{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538626cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch-lightning\n",
    "# !conda install --yes -c conda-forge pytorch-lightning\n",
    "# !conda install --yes -c conda-forge opencv\n",
    "# !conda update pytorch-lightning\n",
    "# !conda uninstall --yes pytorch-lightning\n",
    "# !conda install --yes -c conda-forge pytorch-lightning==0.9\n",
    "# !pip install pytorch-lightning==1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009eeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split,DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from helper import (scale, shift_crop_training_sample,\n",
    "                    crop_sample, NormalizeToTensor,bgr2rgb,rgb2bgr)\n",
    "from datasets import ALOVDataset, ILSVRC2014_DET_Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "# from convlstm import ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c741b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 224\n",
    "# transform = NormalizeToTensor()\n",
    "# data_directory = '../data/'\n",
    "# alov = ALOVDataset(os.path.join(data_directory,\n",
    "#                        'imagedata++/'),\n",
    "#                        os.path.join(data_directory,\n",
    "#                        'alov300++_rectangleAnnotation_full/'),\n",
    "#                        transform, input_size)\n",
    "# alov.show_sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa3e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --load_fast false --logdir=lightning_logs/ --port=8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7616e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,data_directory = '../data/' ,unique_images=4,lambda_shift_frac= 5,lambda_scale_frac= 15,min_scale= -0.4, max_scale= 0.4):\n",
    "        super().__init__()\n",
    "        self.data_directory = data_directory\n",
    "        self.unique_images = unique_images\n",
    "        self.numsynthetic = 10\n",
    "        self.batch_size = self.numsynthetic * self.unique_images + self.unique_images\n",
    "        self.input_size = 224\n",
    "        self.transform = NormalizeToTensor()\n",
    "        self.bb_params = {\n",
    "            'lambda_shift_frac': lambda_shift_frac,\n",
    "            'lambda_scale_frac': lambda_scale_frac,\n",
    "            'min_scale': min_scale,\n",
    "            'max_scale': max_scale\n",
    "        }\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None): \n",
    "        alov = ALOVDataset(os.path.join(self.data_directory,\n",
    "                       'imagedata++/'),\n",
    "                       os.path.join(self.data_directory,\n",
    "                       'alov300++_rectangleAnnotation_full/'),\n",
    "                       self.transform, self.input_size)\n",
    "        alov_len = alov.x.shape[0]\n",
    "        print(\"alov len\", alov_len)\n",
    "        \n",
    "        imagenet = ILSVRC2014_DET_Dataset(os.path.join(self.data_directory,\n",
    "                                                  'ILSVRC2013_DET_val/'),\n",
    "                                                  os.path.join(self.data_directory,\n",
    "                                                  'ILSVRC2013_DET_bbox_val/'),\n",
    "                                                  self.bb_params,\n",
    "                                                  self.transform,\n",
    "                                                  self.input_size)\n",
    "        imagenet_len = imagenet.x.shape[0]\n",
    "        print(\"imgnet len\", imagenet_len)\n",
    "        self.datasets = [alov, imagenet]\n",
    "\n",
    "#         self.datasets = [alov]\n",
    "    \n",
    "        self.lens = [alov_len,imagenet_len]\n",
    "#         self.lens = [alov_len]\n",
    "        \n",
    "        alov_indices = np.arange(alov_len, dtype=np.int32)\n",
    "        np.random.shuffle(alov_indices)\n",
    "        \n",
    "        imagenet_indices = np.arange(imagenet_len,dtype=np.int32)\n",
    "        np.random.shuffle(imagenet_indices)\n",
    "\n",
    "        indices = [alov_indices,imagenet_indices]\n",
    "#         indices = [alov_indices]\n",
    "    \n",
    "        self.train_X = np.array([[i,j] for i in range(len(self.datasets)) for j in indices[i]])\n",
    "#         self.train_y = np.array([self.datasets[i].y[j] for i,j in self.train_X])\n",
    "        train = TensorDataset(torch.tensor(self.train_X))                       \n",
    "        self.track_train,self.track_val,self.track_test = random_split(train,[int(0.8*len(train)),int(0.1*len(train)),len(train)-int(0.8*len(train)) - int(0.1*len(train))])\n",
    "        print(len(self.track_train))\n",
    "        \n",
    "    def make_transformed_samples(self,data):\n",
    "        '''\n",
    "        Given a dataset, it picks a random sample from it and returns a batch\n",
    "        of (kGeneratedExamplesPerImage+1) samples. The batch contains true sample\n",
    "        from dataset and kGeneratedExamplesPerImage samples, which are created\n",
    "        artifically with augmentation by GOTURN smooth motion model.\n",
    "        '''\n",
    "#         print(data)\n",
    "        x1_batch = torch.Tensor(self.batch_size, 3,\n",
    "                                    self.input_size, self.input_size)\n",
    "        x2_batch = torch.Tensor(self.batch_size, 3,\n",
    "                                    self.input_size, self.input_size)\n",
    "        y_batch = torch.Tensor(self.batch_size, 4)\n",
    "        \n",
    "        for pos,tens in enumerate(data):\n",
    "            datasetidx, idx = tens[0][0],tens[0][1]\n",
    "            dataset = self.datasets[datasetidx]\n",
    "            orig_sample = dataset.get_orig_sample(idx)\n",
    "            true_sample, _ = dataset.get_sample(idx)\n",
    "            true_tensor = self.transform(true_sample)\n",
    "#             print(\"true tensor curbb\", true_tensor['currbb'])\n",
    "            # initialize batch with the true sample\n",
    "            temp_pivot = pos*(self.numsynthetic+1)\n",
    "            x1_batch[temp_pivot] = true_tensor['previmg']\n",
    "            x2_batch[temp_pivot] = true_tensor['currimg']\n",
    "            y_batch[temp_pivot] = true_tensor['currbb']\n",
    "\n",
    "            for i in range(self.numsynthetic):\n",
    "                sample = orig_sample\n",
    "                # unscaled current image crop with box\n",
    "                curr_sample, opts_curr = shift_crop_training_sample(sample, self.bb_params)\n",
    "                # unscaled previous image crop with box\n",
    "#                 print(\"curr sample curbb\", curr_sample['bb'])\n",
    "                prev_sample, opts_prev = crop_sample(sample)\n",
    "#                 print(\"prev sample curbb\", prev_sample['bb'])\n",
    "                scaled_curr_obj = scale(curr_sample, opts_curr,self.input_size,self.input_size)\n",
    "                scaled_prev_obj = scale(prev_sample, opts_prev,self.input_size,self.input_size)\n",
    "                training_sample = {'previmg': scaled_prev_obj['image'],\n",
    "                                   'currimg': scaled_curr_obj['image'],\n",
    "                                   'currbb': scaled_curr_obj['bb']}\n",
    "                sample = self.transform(training_sample)\n",
    "                x1_batch[temp_pivot + i + 1] = sample['previmg']\n",
    "                x2_batch[temp_pivot + 1] = sample['currimg']\n",
    "                y_batch[temp_pivot + i + 1] = sample['currbb']\n",
    "        \n",
    "#         print(y_batch)\n",
    "        return x1_batch, x2_batch, y_batch\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.track_train, batch_size=self.unique_images,collate_fn = self.make_transformed_samples, drop_last=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.track_val, batch_size= self.unique_images,collate_fn = self.make_transformed_samples, drop_last=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.track_test,batch_size= self.unique_images,collate_fn = self.make_transformed_samples, drop_last=True)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e411019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker(pl.LightningModule):\n",
    "    \n",
    "  def __init__(self,lr=1e-5,momentum = 0.9,weight_decay = 0.0005,lr_decay= 100000,gamma= 0.1):\n",
    "    super().__init__()\n",
    "    self.lr = lr\n",
    "    self.momentum = momentum\n",
    "    self.weight_decay = weight_decay\n",
    "    self.lr_decay = lr_decay\n",
    "    self.gamma = gamma\n",
    "    caffenet = models.alexnet(pretrained=True)\n",
    "    self.convnet = nn.Sequential(*list(caffenet.children())[:-1])\n",
    "    for param in self.convnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    self.classifier = nn.Sequential(\n",
    "            nn.Linear(256*6*6*2, 4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            )\n",
    "    self.weight_init()\n",
    "    \n",
    "  def weight_init(self):\n",
    "    for m in self.classifier.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.bias.data.fill_(1)\n",
    "            m.weight.data.normal_(0, 0.005)\n",
    "    \n",
    "\n",
    "  def forward(self, x1,x2):\n",
    "    batch_size, channels,width, height = x1.size()\n",
    "    x1 = self.convnet(x1)\n",
    "    x1 = x1.view(batch_size, 256*6*6)\n",
    "    x2 = self.convnet(x2)\n",
    "    x2 = x2.view(batch_size, 256*6*6)\n",
    "    x = torch.cat((x1, x2), 1)\n",
    "    z = self.classifier(x)\n",
    "#     print(\"z\", z)\n",
    "    return z\n",
    "\n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "    x1,x2,y = train_batch\n",
    "#     print(\"Train gt\", y)\n",
    "#     print('-'*120)\n",
    "    logits = self.forward(x1, x2)\n",
    "#     print(\"Logits\", logits)\n",
    "#     print('#'*120)\n",
    "    loss_fn = torch.nn.L1Loss(size_average=False)\n",
    "    loss = loss_fn(logits,y)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "    x1,x2,y = val_batch\n",
    "    logits = self.forward(x1,x2)\n",
    "    loss_fn = torch.nn.L1Loss(size_average=False)\n",
    "    loss = loss_fn(logits,y)\n",
    "#     print(\"y\", y)\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, test_batch, batch_idx):\n",
    "    x1,x2,y = test_batch\n",
    "    logits = self.forward(x1,x2)\n",
    "    loss_fn = torch.nn.L1Loss(size_average=False)\n",
    "    loss = loss_fn(logits,y)\n",
    "    return loss\n",
    "      \n",
    "  def configure_optimizers(self):\n",
    "    optimizer =  torch.optim.SGD(self.parameters(),\n",
    "                          lr=self.lr,\n",
    "                          momentum=self.momentum,\n",
    "                          weight_decay=self.weight_decay)\n",
    "    scheduler =  torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=self.lr_decay,\n",
    "                                          gamma=self.gamma)\n",
    "    return [optimizer],[scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9307f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MemoryNetTracker(pl.LightningModule):\n",
    "    \n",
    "#   def __init__(self,lr=1e-5,momentum = 0.9,weight_decay = 0.0005,lr_decay= 100000,gamma= 0.1):\n",
    "#     super().__init__()\n",
    "#     self.lr = lr\n",
    "#     self.momentum = momentum\n",
    "#     self.weight_decay = weight_decay\n",
    "#     self.lr_decay = lr_decay\n",
    "#     self.gamma = gamma\n",
    "#     resnet = models.resnet50(pretrained=True)\n",
    "#     self.res = nn.Sequential(*list(resnet.children())[:-1])\n",
    "#     for param in self.res.parameters():\n",
    "#         param.requires_grad = False\n",
    "#     self.memory_network = nn.Sequential(\n",
    "#             ConvLSTM(2048*7*7, 4096, 3, 1),\n",
    "#             nn.LSTM(4096, 4096, 2),\n",
    "#             nn.Linear(4096, 4)\n",
    "#             )\n",
    "#     self.proposal_refinement = nn.Sequential(\n",
    "#             nn.Linear(4 + (2048*7*7), 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4)\n",
    "#             )\n",
    "# #     self.weight_init()\n",
    "    \n",
    "#   def weight_init(self):\n",
    "#     for m in self.classifier.modules():\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             m.bias.data.fill_(1)\n",
    "#             m.weight.data.normal_(0, 0.005)\n",
    "    \n",
    "\n",
    "#   def forward(self, x1,x2):\n",
    "#     batch_size, channels, width, height = x1.size()\n",
    "#     x1 = self.res(x1)\n",
    "#     x1_flat = x1.view(batch_size, 2048*7*7)\n",
    "#     x2 = self.res(x2)\n",
    "#     x = torch.cat((x1, x2), 2)\n",
    "#     x_init_pred = self.memory_network(x)\n",
    "#     x_prop = torch.cat((x_init_pred, x1_flat), 1)\n",
    "#     z = self.proposal_refinement(x_prop)\n",
    "#     return z\n",
    "\n",
    "#   def training_step(self, train_batch, batch_idx):\n",
    "#     x1,x2,y = train_batch\n",
    "#     logits = self.forward(x1,x2)\n",
    "#     loss_fn = torch.nn.L1Loss(size_average=False)\n",
    "#     loss = loss_fn(logits,y)\n",
    "#     return loss\n",
    "\n",
    "#   def validation_step(self, val_batch, batch_idx):\n",
    "#     x1,x2,y = val_batch\n",
    "#     logits = self.forward(x1,x2)\n",
    "#     loss_fn = torch.nn.L1Loss(size_average=False)\n",
    "#     loss = loss_fn(logits,y)\n",
    "# #     print(\"y\", y)\n",
    "#     return loss\n",
    "\n",
    "#   def test_step(self, test_batch, batch_idx):\n",
    "#     x1,x2,y = test_batch\n",
    "#     logits = self.forward(x1,x2)\n",
    "#     loss_fn = torch.nn.L1Loss(size_average=False)\n",
    "#     loss = loss_fn(logits,y)\n",
    "#     return loss\n",
    "      \n",
    "#   def configure_optimizers(self):\n",
    "#     optimizer =  torch.optim.SGD(self.parameters(),\n",
    "#                           lr=self.lr,\n",
    "#                           momentum=self.momentum,\n",
    "#                           weight_decay=self.weight_decay)\n",
    "#     scheduler =  torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                           step_size=self.lr_decay,\n",
    "#                                           gamma=self.gamma)\n",
    "#     return [optimizer],[scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc133453",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = TrackDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6a87e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "data_module = TrackDataModule()\n",
    "\n",
    "# train\n",
    "model = Tracker()\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=6,log_every_n_steps=10,accelerator='dp',enable_checkpointing=False)\n",
    "# trainer = pl.Trainer(gpus=3,max_epochs=1,log_every_n_steps=10,enable_checkpointing=True)\n",
    "\n",
    "\n",
    "# trainer.test(datamodule= data_module,model=model)\n",
    "# get_metrics(model.test_labels,model.test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e22d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ALOV dataset...\n",
      "ALOV dataset parsing done.\n",
      "Total number of annotations in ALOV dataset = 19\n",
      "alov len 19\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | convnet    | Sequential | 2.5 M \n",
      "1 | classifier | Sequential | 109 M \n",
      "------------------------------------------\n",
      "109 M     Trainable params\n",
      "2.5 M     Non-trainable params\n",
      "111 M     Total params\n",
      "446.201   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0178616749a44b11b5ecb0c4d4c12f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train gt Train gt Train gt tensor([[ 2.5561,  2.5112,  7.5798,  7.7204],\n",
      "        [ 4.5021,  0.1977,  9.4836,  5.2441],\n",
      "        [ 2.9469,  2.3571,  9.1869,  7.5663],\n",
      "        [ 3.5682,  2.0071,  8.5919,  6.9434],\n",
      "        [ 3.5315,  1.7254,  8.1628,  7.0324],\n",
      "        [ 2.4985, -0.5682,  7.8876,  4.5468],\n",
      "        [ 1.6768,  2.8179,  6.7871,  7.1993],\n",
      "        [ 4.2459,  2.5518,  9.2147,  7.5411],\n",
      "        [ 3.1140,  1.0708,  8.1757,  5.9511],\n",
      "        [ 2.6746,  3.8083,  7.3842,  9.8822],\n",
      "        [ 6.6129,  0.5423, 12.0289,  7.0400],\n",
      "        [ 3.3929,  1.7947,  8.6511,  6.6965],\n",
      "        [ 0.2102,  4.7515,  4.8001,  9.6318],\n",
      "        [ 3.4625,  4.1232,  7.7270,  9.3701],\n",
      "        [ 0.1643,  1.0057,  4.6777,  5.7413]], device='cuda:1')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "tensor([[3.7761, 2.4946, 8.7450, 7.4839],\n",
      "        [2.7232, 3.1555, 8.1939, 7.6081],\n",
      "        [2.7185, 2.1672, 7.3875, 6.8630],\n",
      "        [1.8288, 4.3413, 6.6645, 8.7413],\n",
      "        [3.5353, 2.6175, 8.5501, 7.2742],\n",
      "        [2.6817, 4.7403, 7.2716, 9.1403],\n",
      "        [3.4993, 1.8375, 8.8091, 6.8491],\n",
      "        [2.0528, 4.0033, 6.7623, 9.2015],\n",
      "        [0.4223, 2.0704, 5.8383, 7.8313],\n",
      "        [0.3056, 2.9100, 5.1413, 8.4426],\n",
      "        [3.8161, 2.4695, 8.4059, 7.5961],\n",
      "        [4.2118, 2.3771, 9.1933, 7.3568],\n",
      "        [2.6720, 2.5313, 7.7387, 7.7167],\n",
      "        [2.7389, 4.2173, 7.8492, 8.6852],\n",
      "        [1.3387, 2.1422, 6.1977, 7.0357]], device='cuda:0')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "tensor([[ 5.2796,  3.2693,  9.3826,  8.4917],\n",
      "        [-0.9196,  2.9456,  4.0952,  7.9127],\n",
      "        [ 2.5066,  4.3779,  7.8164,  8.7953],\n",
      "        [ 4.5634,  2.6154,  9.5613,  7.6145],\n",
      "        [ 2.4522,  2.8785,  7.5599,  7.9463],\n",
      "        [ 3.8060,  2.9990, 10.0033,  8.0437],\n",
      "        [ 5.0225,  2.5688,  9.2480,  7.7556],\n",
      "        [ 5.2777,  0.3929,  9.7043,  5.8086],\n",
      "        [-2.6394,  2.2765,  3.9071,  6.4299],\n",
      "        [ 2.6614,  3.1787,  7.1307,  8.2935],\n",
      "        [ 2.5365,  5.0951,  7.1385,  9.4447],\n",
      "        [ 3.7089,  2.6074,  8.5006,  7.7460],\n",
      "        [ 0.8187,  2.0306,  5.8166,  6.6922],\n",
      "        [ 1.9695,  0.8652,  7.3741,  5.4878]], device='cuda:2')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Logits Logits Logits tensor([[1.1405, 1.0616, 0.8133, 1.3564],\n",
      "        [1.1405, 1.0616, 0.8133, 1.3564],\n",
      "        [1.1405, 1.0616, 0.8133, 1.3564],\n",
      "        [1.1405, 1.0616, 0.8133, 1.3564],\n",
      "        [1.1405, 1.0616, 0.8133, 1.3564],\n",
      "        [1.1405, 1.0616, 0.8133, 1.3564],\n",
      "        [1.1405, 1.0616, 0.8133, 1.3564],\n",
      "        [1.1635, 1.0615, 0.7857, 1.3083],\n",
      "        [1.1541, 1.0639, 0.7796, 1.3142],\n",
      "        [1.1457, 1.0462, 0.7944, 1.2998],\n",
      "        [1.1457, 1.0462, 0.7944, 1.2998],\n",
      "        [1.1457, 1.0462, 0.7944, 1.2998],\n",
      "        [1.1457, 1.0462, 0.7944, 1.2998],\n",
      "        [1.1457, 1.0462, 0.7944, 1.2998],\n",
      "        [1.1457, 1.0462, 0.7944, 1.2998]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n",
      "tensor([[1.1457, 1.0462, 0.7944, 1.2998],\n",
      "        [1.1457, 1.0462, 0.7944, 1.2998],\n",
      "        [1.1457, 1.0462, 0.7944, 1.2998],\n",
      "        [1.1060, 1.1108, 0.7872, 1.2696],\n",
      "        [1.1723, 1.0866, 0.7567, 1.3569],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227],\n",
      "        [1.1650, 1.0565, 0.7674, 1.3227]], device='cuda:2',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n",
      "tensor([[1.1836, 1.0731, 0.7949, 1.2784],\n",
      "        [1.1612, 1.1020, 0.7766, 1.3262],\n",
      "        [1.1300, 1.0581, 0.7811, 1.3450],\n",
      "        [1.1300, 1.0581, 0.7811, 1.3450],\n",
      "        [1.1275, 1.0583, 0.7953, 1.3344],\n",
      "        [1.1275, 1.0583, 0.7953, 1.3344],\n",
      "        [1.1275, 1.0583, 0.7953, 1.3344],\n",
      "        [1.1275, 1.0583, 0.7953, 1.3344],\n",
      "        [1.1275, 1.0583, 0.7953, 1.3344],\n",
      "        [1.1275, 1.0583, 0.7953, 1.3344],\n",
      "        [1.1275, 1.0583, 0.7953, 1.3344],\n",
      "        [1.1391, 1.1043, 0.7872, 1.3084],\n",
      "        [1.1285, 1.0670, 0.7913, 1.3729],\n",
      "        [1.1405, 1.0616, 0.8133, 1.3564],\n",
      "        [1.1405, 1.0616, 0.8133, 1.3564]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n",
      "Train gt Train gt Train gt tensor([[ 5.1298,  2.0626, 10.1446,  7.2128],\n",
      "        [ 2.8489,  3.0271,  7.6846,  7.5702],\n",
      "        [ 1.9150,  3.5831,  7.1733,  7.5889],\n",
      "        [ 1.3375,  1.0372,  6.0883,  5.9606],\n",
      "        [ 2.7408, -0.5925,  7.3698,  4.3968],\n",
      "        [ 1.9163,  3.1452,  8.0017,  8.1345],\n",
      "        [ 2.5204,  2.6840,  8.2821,  7.2643],\n",
      "        [ 4.1284,  2.6089,  9.0972,  7.5982],\n",
      "        [ 3.5074, -0.7560,  8.8698,  3.8813],\n",
      "        [ 1.5095,  3.3267,  6.3452,  7.9259],\n",
      "        [ 2.5325,  3.2397,  7.6420,  8.3663],\n",
      "        [ 2.1539,  1.2391,  7.3616,  6.4860],\n",
      "        [ 2.4930,  3.4260,  7.2439,  8.2851],\n",
      "        [ 5.1375,  1.2345, 10.2469,  6.4569],\n",
      "        [ 2.6921,  3.6646,  7.2433,  8.5450]], device='cuda:1')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "tensor([[ 2.4795,  3.1490,  6.8827,  8.5221],\n",
      "        [ 2.6388,  3.5447,  8.5904,  7.7621],\n",
      "        [ 2.5982,  2.0162,  7.6599,  7.5489],\n",
      "        [ 4.2882,  2.5463,  9.6989,  7.4696],\n",
      "        [ 1.7412,  3.1047,  7.0381,  8.1847],\n",
      "        [ 1.8131,  3.5304,  6.7953,  8.3896],\n",
      "        [ 1.7487,  2.4656,  6.3232,  7.1223],\n",
      "        [ 2.2744,  0.9803,  7.5712,  7.1549],\n",
      "        [ 2.8688,  3.6768,  8.9315,  8.2571],\n",
      "        [ 6.9125,  2.2622, 11.6153,  6.7506],\n",
      "        [ 2.8983,  2.6989,  7.5575,  7.7559],\n",
      "        [ 1.4934,  3.1853,  6.4756,  8.3119],\n",
      "        [ 1.6964,  1.8677,  7.4803,  6.5635],\n",
      "        [ 3.6733,  2.8872,  8.7053,  7.6228]], device='cuda:2')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "tensor([[ 4.7011,  2.4946, 10.5247,  7.4839],\n",
      "        [ 2.2339,  2.3552,  7.3433,  8.2373],\n",
      "        [ 2.4220,  2.2771,  7.3013,  7.4996],\n",
      "        [ 2.4578,  1.9521,  7.8202,  7.6252],\n",
      "        [ 2.9680,  1.4289,  7.6370,  6.3960],\n",
      "        [ 1.9519,  1.9177,  6.7028,  7.0209],\n",
      "        [ 2.4090,  2.6698,  7.1185,  7.6591],\n",
      "        [ 1.7166,  3.0303,  6.7314,  7.6106],\n",
      "        [ 2.4074,  1.1041,  7.1583,  6.4260],\n",
      "        [ 2.7401,  2.3113,  7.3692,  7.3683],\n",
      "        [-2.2190,  1.9933,  3.5427,  6.8950],\n",
      "        [ 4.2459,  2.5518,  9.2147,  7.5411],\n",
      "        [ 2.0177,  2.8148,  7.1758,  7.6321],\n",
      "        [ 2.1697,  1.5026,  6.5374,  6.6767],\n",
      "        [ 3.0055,  2.2537,  8.2132,  6.8529]], device='cuda:0')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Logits Logits Logits tensor([[52.3203, 55.8161, 76.5821, 77.0446],\n",
      "        [52.3203, 55.8161, 76.5821, 77.0446],\n",
      "        [52.3203, 55.8161, 76.5821, 77.0446],\n",
      "        [52.3203, 55.8161, 76.5821, 77.0446],\n",
      "        [52.3203, 55.8161, 76.5821, 77.0446],\n",
      "        [52.3203, 55.8161, 76.5821, 77.0446],\n",
      "        [52.3203, 55.8161, 76.5821, 77.0446],\n",
      "        [52.5012, 56.0531, 76.7907, 77.2828],\n",
      "        [52.4288, 55.9316, 76.6802, 77.2064],\n",
      "        [52.3344, 55.8191, 76.6122, 77.1050],\n",
      "        [52.3344, 55.8191, 76.6122, 77.1050],\n",
      "        [52.3344, 55.8191, 76.6122, 77.1050],\n",
      "        [52.3344, 55.8191, 76.6122, 77.1050],\n",
      "        [52.3344, 55.8191, 76.6122, 77.1050],\n",
      "        [52.3344, 55.8191, 76.6122, 77.1050]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n",
      "tensor([[52.3344, 55.8191, 76.6122, 77.1050],\n",
      "        [52.3344, 55.8191, 76.6122, 77.1050],\n",
      "        [52.3344, 55.8191, 76.6122, 77.1050],\n",
      "        [52.3444, 55.8792, 76.5895, 77.0424],\n",
      "        [52.3721, 55.8989, 76.6001, 77.1063],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607],\n",
      "        [52.3141, 55.7972, 76.5748, 77.0607]], device='cuda:2',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n",
      "tensor([[52.2765, 55.8319, 76.4699, 76.9256],\n",
      "        [52.3475, 55.7939, 76.5620, 77.0058],\n",
      "        [52.2962, 55.7419, 76.4924, 76.9779],\n",
      "        [52.2962, 55.7419, 76.4924, 76.9779],\n",
      "        [52.2931, 55.7397, 76.5216, 76.9944],\n",
      "        [52.2931, 55.7397, 76.5216, 76.9944],\n",
      "        [52.2931, 55.7397, 76.5216, 76.9944],\n",
      "        [52.2931, 55.7397, 76.5216, 76.9944],\n",
      "        [52.2931, 55.7397, 76.5216, 76.9944],\n",
      "        [52.2931, 55.7397, 76.5216, 76.9944],\n",
      "        [52.2931, 55.7397, 76.5216, 76.9944],\n",
      "        [52.4799, 55.9907, 76.7504, 77.2791],\n",
      "        [52.4942, 55.9997, 76.7783, 77.2620],\n",
      "        [52.3203, 55.8161, 76.5821, 77.0446],\n",
      "        [52.3203, 55.8161, 76.5821, 77.0446]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train gt Train gt Train gt tensor([[ 3.4239,  2.4946,  8.3927,  7.4839],\n",
      "        [ 3.0657,  2.3138,  7.8166,  7.0096],\n",
      "        [ 6.2852,  3.1513, 11.2089,  6.8520],\n",
      "        [ 2.9409,  2.5541,  7.4922,  7.6573],\n",
      "        [ 6.9984,  2.8393, 12.2567,  7.4575],\n",
      "        [ 2.4979,  3.5272,  7.5127,  8.3653],\n",
      "        [ 2.1243,  3.2137,  7.0036,  8.1808],\n",
      "        [ 3.1589,  3.4708,  6.7221,  7.7202],\n",
      "        [ 0.6772,  3.9346,  6.1479,  8.8363],\n",
      "        [ 3.0022,  2.3212,  7.7531,  7.3329],\n",
      "        [ 2.3177,  6.8709,  7.5254, 11.4512],\n",
      "        [ 4.1284,  2.6089,  9.0972,  7.5982],\n",
      "        [ 2.6813,  3.4363,  7.6961,  8.6587],\n",
      "        [ 1.8708,  2.2079,  6.7944,  7.1096],\n",
      "        [ 3.1157,  2.2797,  8.2738,  7.4063]], device='cuda:0')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "tensor([[ 5.1545,  2.3147, 10.8928,  7.2380],\n",
      "        [ 3.3821,  4.7650,  8.3268,  9.5006],\n",
      "        [-0.8887, -0.9911,  4.0040,  4.2071],\n",
      "        [ 3.8753,  2.4995, 10.2495,  7.6145],\n",
      "        [ 0.3594,  6.2128,  5.2586, 10.6113],\n",
      "        [ 1.9611,  0.4126,  6.8602,  5.4366],\n",
      "        [ 2.2586,  2.4690,  7.6971,  7.7023],\n",
      "        [ 2.5483,  2.5823,  7.2530,  7.6514],\n",
      "        [ 2.6851,  1.7747,  7.1090,  6.7110],\n",
      "        [ 4.9636,  6.1056,  9.9451, 10.5735],\n",
      "        [ 2.7098,  2.0748,  7.2350,  7.4068],\n",
      "        [ 5.0925,  1.0993, 10.3385,  6.6953],\n",
      "        [ 1.4042,  2.5094,  6.1090,  6.9424],\n",
      "        [ 0.9106,  1.1755,  6.3491,  6.3608]], device='cuda:2')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "tensor([[4.0714, 2.3137, 9.0402, 7.2589],\n",
      "        [1.6296, 0.6594, 6.2986, 5.8576],\n",
      "        [1.5340, 4.0651, 7.1756, 9.0323],\n",
      "        [0.9129, 2.5961, 6.6746, 7.3518],\n",
      "        [2.3364, 0.4174, 6.9263, 5.4744],\n",
      "        [2.7754, 4.7257, 7.7903, 9.8759],\n",
      "        [2.1530, 0.8646, 8.2384, 5.9678],\n",
      "        [4.2655, 2.4946, 8.2724, 7.4839],\n",
      "        [0.4177, 2.8158, 5.4155, 7.4532],\n",
      "        [2.1643, 3.6138, 7.5690, 7.5079],\n",
      "        [3.0717, 3.2285, 7.8634, 8.5759],\n",
      "        [2.0857, 2.5666, 6.6877, 7.4047],\n",
      "        [4.8733, 2.1443, 9.7150, 6.7816],\n",
      "        [3.1878, 2.4503, 7.8828, 7.3094],\n",
      "        [2.6737, 2.1803, 8.0783, 6.7984]], device='cuda:1')\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Logits Logits Logits tensor([[0.8780, 1.0349, 1.4621, 1.4644],\n",
      "        [0.8739, 1.0292, 1.4621, 1.4701],\n",
      "        [0.8662, 1.0371, 1.4852, 1.4784],\n",
      "        [0.8662, 1.0371, 1.4852, 1.4784],\n",
      "        [0.8672, 1.0360, 1.4777, 1.4723],\n",
      "        [0.8672, 1.0360, 1.4777, 1.4723],\n",
      "        [0.8672, 1.0360, 1.4777, 1.4723],\n",
      "        [0.8672, 1.0360, 1.4777, 1.4723],\n",
      "        [0.8672, 1.0360, 1.4777, 1.4723],\n",
      "        [0.8672, 1.0360, 1.4777, 1.4723],\n",
      "        [0.8672, 1.0360, 1.4777, 1.4723],\n",
      "        [0.8582, 1.0179, 1.4469, 1.4421],\n",
      "        [0.8889, 1.0491, 1.4951, 1.4955],\n",
      "        [0.8899, 1.0559, 1.5101, 1.5067],\n",
      "        [0.8899, 1.0559, 1.5101, 1.5067]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n",
      "tensor([[0.8769, 1.0431, 1.4851, 1.4808],\n",
      "        [0.8769, 1.0431, 1.4851, 1.4808],\n",
      "        [0.8769, 1.0431, 1.4851, 1.4808],\n",
      "        [0.8635, 1.0204, 1.4577, 1.4521],\n",
      "        [0.8637, 1.0257, 1.4627, 1.4664],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740],\n",
      "        [0.8678, 1.0383, 1.4738, 1.4740]], device='cuda:2',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n",
      "tensor([[0.8899, 1.0559, 1.5101, 1.5067],\n",
      "        [0.8899, 1.0559, 1.5101, 1.5067],\n",
      "        [0.8899, 1.0559, 1.5101, 1.5067],\n",
      "        [0.8899, 1.0559, 1.5101, 1.5067],\n",
      "        [0.8899, 1.0559, 1.5101, 1.5067],\n",
      "        [0.8899, 1.0559, 1.5101, 1.5067],\n",
      "        [0.8899, 1.0559, 1.5101, 1.5067],\n",
      "        [0.8828, 1.0482, 1.4821, 1.4918],\n",
      "        [0.8832, 1.0471, 1.4781, 1.4741],\n",
      "        [0.8769, 1.0431, 1.4851, 1.4808],\n",
      "        [0.8769, 1.0431, 1.4851, 1.4808],\n",
      "        [0.8769, 1.0431, 1.4851, 1.4808],\n",
      "        [0.8769, 1.0431, 1.4851, 1.4808],\n",
      "        [0.8769, 1.0431, 1.4851, 1.4808],\n",
      "        [0.8769, 1.0431, 1.4851, 1.4808]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "########################################################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, data_module)\n",
    "trainer.save_checkpoint(\"train_model_drop.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a9cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = Tracker.load_from_checkpoint(checkpoint_path=\"train_model_drop.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db817364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09c73f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = module.train_dataloader()\n",
    "# for ex in tqdm(loader):\n",
    "#     print(ex[0].shape,ex[1].shape,ex[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45cf4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module.train_y[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2907642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(args.manual_seed)\n",
    "# torch.manual_seed(args.manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee2af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# checkpoint_steps = 20000  # save model after every 20000 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9237b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([\"kawshik\",\"gayatri\",\"gokul\"])\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7145270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = TensorDataset(torch.Tensor(np.array([[1,2,3],[4,5,6]])),torch.Tensor(np.array([[1,2,3],[4,5,6]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6833ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shuffle()\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd35018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alov_len = 10\n",
    "# imagenet_len = 15\n",
    "# datasets = [1,2]\n",
    "# a = np.arange(alov_len,dtype=np.int32)\n",
    "# np.random.shuffle(a)\n",
    "# b = np.arange(imagenet_len,dtype=np.int32)\n",
    "# np.random.shuffle(b)\n",
    "# indices = [a,b]\n",
    "# print(indices)\n",
    "# train_X = np.array([[i,j] for i in range(len(datasets)) for j in indices[i]])\n",
    "# print(train_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "610d0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda60b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
